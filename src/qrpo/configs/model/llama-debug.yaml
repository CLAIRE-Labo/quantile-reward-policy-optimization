# @package _global_

model_args:
  torch_dtype: bfloat16
  model_name_or_path: meta-llama/Llama-3.2-1B-Instruct
  attn_implementation: flash_attention_2
  trust_remote_code: true
  use_peft: false

tokenizer_args:
  pad_token_id: 128004  #  '<|finetune_right_pad_id|>'

model_generation_config:
  temperature: 1
  top_p: 1


model_eval_generation_config:
  # From model config, same as generation_config.json
  temperature: 1
  top_p: 1
